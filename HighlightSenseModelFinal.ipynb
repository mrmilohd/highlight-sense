{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru4m3zLBP4Yy"
      },
      "source": [
        "**MODEL PIPELINE** This is a CRNN based model with an attention layer, which takes as input clips, in form of log mel spectrograms, with the game audio and the streamer audio stacked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgHO-5YoHueY"
      },
      "source": [
        "**1. Downloading the custom Dataset from Kaggle:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "TO GET THE DATASET GO TO KAGGLE SETTINGS AND DOWNLOAD\n",
        "THE API KEY FILE(JSON), UNDER THE LEGACY API CREDENTIALS AND THEN UPLOAD IT TO COLAB LOCAL STORAGE MAKE SURE THE FOLDER IT IS UNDER MATCHES THE FOLDER PATH IN THE FIRST CELL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFWZG9PilHWR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# the folder containing the key\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "\n",
        "# download dataset\n",
        "path = kagglehub.dataset_download(\"aadityahh/dataset-highlight-sense\")\n",
        "\n",
        "print(\"Dataset path:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBGmMmn4Iex2"
      },
      "source": [
        "**2. CONFIGURATION** the output_dir stores the path of the trained model after training is done, all the hyperparameters are listed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OXegPOnz04R"
      },
      "outputs": [],
      "source": [
        "base_path = path # after copying files from kaggle to local ssd for faster processing\n",
        "output_dir='/content/best_model.pt'\n",
        "# log mel spectrograms specification\n",
        "n_mels= 80\n",
        "hop_length= 512\n",
        "sample_rate= 22050\n",
        "# model specifications\n",
        "max_duration_sec= 60.0\n",
        "max_time_frames= 2584 # Calculated as int(max_duration_sec * sample_rate / hop_length)\n",
        "rnn_dropout= 0.3\n",
        "dropout= 0.5\n",
        "batch_size= 64\n",
        "lr= 1e-4\n",
        "weight_decay= 1e-5\n",
        "epochs= 10\n",
        "val_split= 0.15\n",
        "grad_clip: float = 1.0 # maxxed at 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ItVYS7ioB8"
      },
      "source": [
        "**3. Import required dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anh7qbMmiqfG"
      },
      "outputs": [],
      "source": [
        "import os, zipfile, random, gc, time, io\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WARcEvfi0U0"
      },
      "source": [
        "**4. Indexing Dataset** Appending the paths as well as labels to the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26V9jpiQi5pG"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_data_index(base_path_str):\n",
        "    index = {\n",
        "        'positive': [],\n",
        "        'negative': []\n",
        "    }\n",
        "\n",
        "    base_path = Path(base_path_str)\n",
        "\n",
        "    pos_path = (base_path / 'Positive').rglob('*.pt')\n",
        "    index['positive'] = [str(p) for p in pos_path]\n",
        "\n",
        "    neg_path = (base_path / 'Negative').rglob('*.pt')\n",
        "    index['negative'] = [str(p) for p in neg_path]\n",
        "\n",
        "    return index\n",
        "\n",
        "print(\"getting data index\")\n",
        "data_index_final = get_data_index(base_path)\n",
        "print(f\"Found {len(data_index_final['positive'])} positive and {len(data_index_final['negative'])} negative files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SU8i--2i6qF"
      },
      "source": [
        "**5. Train Val Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0s7Utphi_2e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "all_files=data_index_final['positive']+data_index_final['negative']\n",
        "all_labels=[1]*len(data_index_final['positive'])+[0]*len(data_index_final['negative'])\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    all_files, all_labels, test_size=val_split, stratify=all_labels, random_state=42\n",
        ")\n",
        "print(f\"no. of training files {len(train_files)}, no. of validation files {len(val_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rgQunNZjNv4"
      },
      "source": [
        "**6. Dataset and Augmentations** since this model contains an attention layer at end, there are a few steps involved in this phase, first for positive clips, a mask is generated with the pad value of the lowest energy moment in the clip, to pad the clip to the length of 60 second. Now the negative clips are all 30 seconds each, to not allow the model to cheat on the basis of mask length, I got the probablity distribution of the positive clips, and augmented the negative clips on that basis- by either cropping the clips or by adding a random chunk at the end. Then using the same padding logic as positive clips,along with this a volume augmentation is applied, so the model doesnt dumb learn that high volume= highlight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGgUFcpDjRyi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from typing import List\n",
        "import random\n",
        "\n",
        "class HighlightSenseDataset(Dataset):\n",
        "    def __init__(self, file_paths: List[str], labels: List[int], max_frames: int = 2584,training=True):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.max_frames = max_frames\n",
        "        self.training=training\n",
        "        # Hardcoded probability distribution of positive samples\n",
        "        self.pos_distribution = [\n",
        "            (215, 430, 0.114),    # 0-10s\n",
        "            (431, 861, 0.227),    # 10-20s\n",
        "            (862, 1292, 0.279),   # 20-30s\n",
        "            (1293, 1722, 0.184),  # 30-40s\n",
        "            (1723, 2153, 0.072),  # 40-50s\n",
        "            (2154, 2584, 0.124),  # 50-60s+\n",
        "        ]\n",
        "\n",
        "    def sample_frame_count(self):\n",
        "        rand_val = random.random()\n",
        "        cumulative = 0.0\n",
        "        for start, end, prob in self.pos_distribution:\n",
        "            cumulative += prob\n",
        "            if rand_val <= cumulative:\n",
        "                return random.randint(start, end)\n",
        "        return 2584  # fallback\n",
        "\n",
        "    def pad_or_truncate(self, input_tensor):\n",
        "        num_frames = input_tensor.shape[2]\n",
        "        mask = torch.ones(self.max_frames)\n",
        "\n",
        "        if num_frames >= self.max_frames:\n",
        "            output_tensor = input_tensor[:, :, :self.max_frames]\n",
        "            return output_tensor, mask\n",
        "        else:\n",
        "            pad_size = self.max_frames - num_frames\n",
        "            silence_val = input_tensor.min().item()\n",
        "            pad_tensor = F.pad(input_tensor, (0, pad_size), value=silence_val)\n",
        "            mask[num_frames:] = 0\n",
        "\n",
        "            return pad_tensor, mask\n",
        "\n",
        "    def augment_negative_tensor(self, input_tensor):\n",
        "        num_frames = input_tensor.shape[2]\n",
        "        final_frames = self.sample_frame_count()\n",
        "\n",
        "        if final_frames <= num_frames:\n",
        "            # Crop to shorter length\n",
        "            start = random.randint(0, num_frames - final_frames)\n",
        "            return input_tensor[:, :, start:start + final_frames]\n",
        "        else:\n",
        "            # Need more frames - repeat a chunk\n",
        "            need = final_frames - num_frames\n",
        "            chunk_size = min(need, num_frames)\n",
        "            start = random.randint(0, num_frames - chunk_size)\n",
        "            extra = input_tensor[:, :, start:start + chunk_size]\n",
        "            augmented_tensor = torch.cat((input_tensor, extra), dim=2)\n",
        "            return augmented_tensor[:, :, :final_frames]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        data = torch.load(file_path)\n",
        "        input_tensor = data['tensor']\n",
        "        input_tensor=input_tensor.float()\n",
        "        if self.training:\n",
        "          scale = random.uniform(0.5, 1.5)\n",
        "          input_tensor=input_tensor*scale\n",
        "\n",
        "\n",
        "        if label == 1:\n",
        "            input_tensor, mask = self.pad_or_truncate(input_tensor)\n",
        "        else:\n",
        "          if self.training:\n",
        "            input_tensor = self.augment_negative_tensor(input_tensor)\n",
        "          input_tensor, mask = self.pad_or_truncate(input_tensor)\n",
        "\n",
        "        return input_tensor, mask, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUd8SU2R1pK2"
      },
      "source": [
        "**7. Dataloader** used weighted random sampling since the dataset was a little biased, since negative clips were more- 1:1.2 ratio was present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgynUB7d-akf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = HighlightSenseDataset(train_files, train_labels, max_time_frames)\n",
        "val_ds   = HighlightSenseDataset(val_files, val_labels, max_time_frames, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHsZeixj2oM6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "labels = torch.tensor(train_ds.labels)\n",
        "\n",
        "class_counts = torch.bincount(labels)\n",
        "class_weights = 1.0 / class_counts.float()\n",
        "\n",
        "sample_weights = class_weights[labels]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXDqQrWx1qaw"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=32,\n",
        "    sampler=sampler\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIsP-CdMMkUs"
      },
      "source": [
        "**8. Model Architecture** The model consists of CNN+RNN+Attention+Classifier, 4 layers of CRNN, 2 layers of RNN, an attention layer and a classifier with 2 linear layers. To further prevent dumb learning volume- instance norm is useed on the first layer for per clip normalization to remove any volume bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20hFyu1n-rkh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#convblock\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, inc, outc, pool=(2,2), instance_norm=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(inc, outc, 3, padding=1)\n",
        "\n",
        "        if instance_norm:\n",
        "            self.norm = nn.InstanceNorm2d(outc, affine=True)\n",
        "        else:\n",
        "            self.norm = nn.BatchNorm2d(outc)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(pool)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, att_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, att_dim)\n",
        "        self.fc2 = nn.Linear(att_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        e = torch.tanh(self.fc1(x))\n",
        "        s = self.fc2(e).squeeze(-1)\n",
        "        if mask is not None:\n",
        "            s = s.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        w = torch.softmax(s, dim=1)\n",
        "        ctx = torch.bmm(w.unsqueeze(1), x).squeeze(1)\n",
        "        return ctx, w\n",
        "\n",
        "\n",
        "# MAIN MODEL\n",
        "class HighlightCRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_mels=80,\n",
        "        cnn_channels=[32,64,128,256],\n",
        "        rnn_hidden=256,\n",
        "        rnn_layers=2,\n",
        "        rnn_dropout=0.3,\n",
        "        att_dim=128,\n",
        "        dropout=0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        #CNN\n",
        "        layers = []\n",
        "        in_ch = 2\n",
        "\n",
        "        for i, out_ch in enumerate(cnn_channels):\n",
        "            use_instance_norm = (i == 0)     # only first layer\n",
        "            pool = (2,2) if i < 3 else (1,2)\n",
        "\n",
        "            layers.append(\n",
        "                ConvBlock(in_ch, out_ch, pool, instance_norm=use_instance_norm)\n",
        "            )\n",
        "            in_ch = out_ch\n",
        "\n",
        "        self.cnn = nn.Sequential(*layers)\n",
        "\n",
        "        # after pooling freq size reduces by 8\n",
        "        freq_out = n_mels // 8\n",
        "        cnn_feat_dim = cnn_channels[-1] * freq_out\n",
        "\n",
        "        #RNN\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=cnn_feat_dim,\n",
        "            hidden_size=rnn_hidden,\n",
        "            num_layers=rnn_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=rnn_dropout if rnn_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        #Attention\n",
        "        self.attn = Attention(rnn_hidden * 2, att_dim)\n",
        "\n",
        "        #Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_hidden * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout / 2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B = x.size(0)\n",
        "\n",
        "        #CNN\n",
        "        c = self.cnn(x)\n",
        "        c = c.permute(0, 3, 1, 2)\n",
        "        c = c.reshape(B, c.size(1), -1)\n",
        "\n",
        "        #Mask\n",
        "        if mask is not None:\n",
        "            m = mask[:, ::16]\n",
        "            if m.size(1) != c.size(1):\n",
        "                m = F.pad(m, (0, c.size(1) - m.size(1)))[:, :c.size(1)]\n",
        "        else:\n",
        "            m = None\n",
        "        #RNN\n",
        "        rnn_out, _ = self.rnn(c)\n",
        "\n",
        "        #Attention\n",
        "        ctx, att = self.attn(rnn_out, m)\n",
        "\n",
        "        #Classifier\n",
        "        out = self.classifier(ctx)\n",
        "\n",
        "        return out, att\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5pLBjx1P5fi"
      },
      "source": [
        "Simple checking wether the model works it should output a simple (4,1) shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLqlVCkWSRRx"
      },
      "outputs": [],
      "source": [
        "#verify if model works\n",
        "n_mels = 80\n",
        "cnn_channels = [32, 64, 128, 256]\n",
        "rnn_hidden = 256\n",
        "rnn_layers = 2\n",
        "rnn_dropout = 0.3\n",
        "attention_dim = 128\n",
        "dropout = 0.5\n",
        "\n",
        "model = HighlightCRNN(\n",
        "    n_mels,\n",
        "    cnn_channels,\n",
        "    rnn_hidden,\n",
        "    rnn_layers,\n",
        "    rnn_dropout,\n",
        "    attention_dim,\n",
        "    dropout\n",
        ").to(device)\n",
        "\n",
        "\n",
        "x = torch.randn(4, 2, 80, 2584).to(device)\n",
        "m = torch.ones(4, 2584).to(device)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out, att = model(x, m)\n",
        "    print(out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDod-IPZQIZl"
      },
      "source": [
        "**9. Training and evaluation** using simple BCELoss and using AdamW optimizer which also incorporates L2 regularization, along with this another regularization used is gradient clipping, with limit set at 1.0, the model is saved as a dictionary containing the number of epochs, and the model weights. The Val set runs just after the training set and calculates loss, the f1 and and precision score @0.5 threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDMZu3pBavGf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "def train_and_eval(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-5,\n",
        "    epochs=epochs,\n",
        "    grad_clip=1,\n",
        "    pos_weight=None,\n",
        "    save_path = None\n",
        "\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    #weighted class since data imbalanced\n",
        "    if pos_weight is not None:\n",
        "        pw = torch.tensor([pos_weight], device=device)\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    #TRAIN\n",
        "    print(\"started training\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\") as progress_bar:\n",
        "          for specs, masks, labels in train_loader:\n",
        "            specs = specs.to(device)\n",
        "            masks = masks.to(device)\n",
        "            labels = labels.unsqueeze(1).to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, _ = model(specs, masks)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            current_loss=loss.item()\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=f\"{current_loss:.4f}\")\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} | Train Loss = {train_loss:.4f}\")\n",
        "\n",
        "    #Save Model\n",
        "    checkpoint = {\n",
        "        'model': model.state_dict(),\n",
        "        'epochs': epochs\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "\n",
        "    # ValSet\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for specs, masks, labels in val_loader:\n",
        "            specs = specs.to(device)\n",
        "            masks = masks.to(device)\n",
        "            labels = labels.to(device).unsqueeze(1).float()\n",
        "\n",
        "            logits, _ = model(specs, masks)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    preds = torch.cat(all_preds).numpy().flatten()\n",
        "    labels = torch.cat(all_labels).numpy().flatten()\n",
        "\n",
        "    tp = ((preds == 1) & (labels == 1)).sum()\n",
        "    fp = ((preds == 1) & (labels == 0)).sum()\n",
        "    fn = ((preds == 0) & (labels == 1)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    print(f\"Val Loss : {val_loss:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"F1 Score : {f1:.4f}\")\n",
        "\n",
        "    return val_loss, precision, f1\n",
        "\n",
        "\n",
        "loss_final, precision,f1=train_and_eval(model,train_loader,val_loader,device,lr=1e-4,weight_decay=1e-5,epochs=epochs,grad_clip=1.0,pos_weight=None,save_path=os.path.join(output_dir, \"best_model.pt\"))\n",
        "print(loss_final)\n",
        "print(precision)\n",
        "print(f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJFHxGzbSDGy"
      },
      "source": [
        "**10. Cleanup** After training and eval the model hogs up ram and gpu, to clean it run this function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn39hn6OSCNs"
      },
      "outputs": [],
      "source": [
        "#deleting memory for rerun\n",
        "import gc\n",
        "if 'model' in globals(): del model\n",
        "if 'optimizer' in globals(): del optimizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = HighlightCRNN(n_mels, cnn_channels, rnn_hidden, rnn_layers,\n",
        "                      rnn_dropout, attention_dim, dropout)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
