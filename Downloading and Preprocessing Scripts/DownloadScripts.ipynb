{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Scripts, generated by AI**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This downloads the clips in form of .m4a file. For downloading the top clips, we use 5 year data, for each week we get the top 100 clips, and run this for the entire time duration, and then sort by views, and then download the required amount of clips. MultiThreading is used for parallel downloading. For downloading the negative dataset, since VODs are cleared out after max of 60 days by Twitch, we sort by views the top available VODs, get metadata of the top clips of that stream, and download around 50 clips of 30 second each, avoiding the clipped regions. the outputs are moved to gdrive in batches, each batch contains a metadata file containing the specifications of each and every clip- including view count streamer_id, start time, end time, game id. This is possible through Twitch API, which requires the ID and secret code, generated through twitch developers console."
      ],
      "metadata": {
        "id": "kzfrXzM41Ls0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google Drive for Access"
      ],
      "metadata": {
        "id": "zhzx6PWQ1kVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7VjKZoXCkJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ea7f71-52ff-4125-f1fe-f8192378db2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "cqIX4Hqt1nnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz3ZxWvEFFdP"
      },
      "outputs": [],
      "source": [
        "!apt-get -y install ffmpeg\n",
        "!pip install yt-dlp requests tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading positive clips (i had downloaded game wise, here is the code which i had used for CS2 clips, have done similar for Apex Legends and Valorant)"
      ],
      "metadata": {
        "id": "LEtQYA9j178x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import time\n",
        "import logging\n",
        "import yt_dlp\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "class Config:\n",
        "    \"\"\"Central configuration for the dataset collector.\"\"\"\n",
        "    # Twitch API Credentials\n",
        "    CLIENT_ID = '4ursdzwlj6dbb35hxij9cnooap044r'\n",
        "    CLIENT_SECRET = '4g8dimywmzmzw1e2aja3uabagvkxv2'\n",
        "\n",
        "    # Target Game: Counter-Strike (CS:GO & CS2)\n",
        "    GAME_ID = \"32399\"\n",
        "\n",
        "    # Data Collection Goals\n",
        "    TOTAL_TARGET_CLIPS = 12000\n",
        "    LOOKBACK_YEARS = 5\n",
        "\n",
        "    # Batch Processing Settings\n",
        "    BATCH_SIZE = 500         # Number of clips to process before zipping/uploading\n",
        "    MAX_WORKERS = 16         # Number of parallel download threads\n",
        "\n",
        "    # Directories (optimized for Google Colab)\n",
        "    BASE_DIR = \"/content/cs2_audio_batches\"\n",
        "    DRIVE_FOLDER = \"/content/drive/MyDrive/CS2_Dataset_12k\"\n",
        "\n",
        "# Configure Logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN CLASS\n",
        "# ==============================================================================\n",
        "class TwitchAudioCollector:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize directories and setup.\"\"\"\n",
        "        self.headers = None\n",
        "        self._setup_directories()\n",
        "\n",
        "    def _setup_directories(self):\n",
        "        \"\"\"Ensures the output directories exist.\"\"\"\n",
        "        if not os.path.exists(Config.DRIVE_FOLDER):\n",
        "            os.makedirs(Config.DRIVE_FOLDER)\n",
        "            logger.info(f\"Created Drive folder: {Config.DRIVE_FOLDER}\")\n",
        "\n",
        "        if not os.path.exists(Config.BASE_DIR):\n",
        "            os.makedirs(Config.BASE_DIR)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 1. AUTHENTICATION\n",
        "    # --------------------------------------------------------------------------\n",
        "    def authenticate(self):\n",
        "        \"\"\"\n",
        "        Obtains an OAuth 2.0 Access Token from Twitch.\n",
        "        Raises an exception if authentication fails.\n",
        "        \"\"\"\n",
        "        url = \"https://id.twitch.tv/oauth2/token\"\n",
        "        params = {\n",
        "            \"client_id\": Config.CLIENT_ID,\n",
        "            \"client_secret\": Config.CLIENT_SECRET,\n",
        "            \"grant_type\": \"client_credentials\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, params=params)\n",
        "            response.raise_for_status() # Raise error for 4xx/5xx responses\n",
        "            data = response.json()\n",
        "\n",
        "            self.headers = {\n",
        "                \"Client-ID\": Config.CLIENT_ID,\n",
        "                \"Authorization\": f\"Bearer {data['access_token']}\"\n",
        "            }\n",
        "            logger.info(\"âœ… Twitch Authentication Successful\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Authentication Failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 2. METADATA SCANNING\n",
        "    # --------------------------------------------------------------------------\n",
        "    def fetch_historical_metadata(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Performs a 'Smart Scan' of the last N years.\n",
        "        Because Twitch API doesn't allow sorting by 'all time' deep in the past,\n",
        "        we iterate through small time windows (7 days) to gather high-density data.\n",
        "        \"\"\"\n",
        "        logger.info(f\"--- Step 1: Scanning {Config.LOOKBACK_YEARS} Years of History ---\")\n",
        "\n",
        "        clips = []\n",
        "        end_date = datetime.now(timezone.utc)\n",
        "        start_date = end_date - timedelta(days=365 * Config.LOOKBACK_YEARS)\n",
        "\n",
        "        current_window_start = start_date\n",
        "        total_weeks = int((end_date - start_date).days / 7)\n",
        "\n",
        "        # tqdm creates a visual progress bar\n",
        "        with tqdm(total=total_weeks, desc=\"Scanning Timeline\", unit=\"week\") as pbar:\n",
        "            while current_window_start < end_date:\n",
        "                # Stop if we have enough raw candidates (1.5x buffer for deduplication)\n",
        "                if len(clips) > (Config.TOTAL_TARGET_CLIPS * 1.5):\n",
        "                    break\n",
        "\n",
        "                # Define a 7-day window\n",
        "                window_end = min(current_window_start + timedelta(days=7), end_date)\n",
        "\n",
        "                params = {\n",
        "                    \"game_id\": Config.GAME_ID,\n",
        "                    \"started_at\": current_window_start.isoformat(),\n",
        "                    \"ended_at\": window_end.isoformat(),\n",
        "                    \"first\": 100  # Max allowed by Twitch per request\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    response = requests.get(\n",
        "                        \"https://api.twitch.tv/helix/clips\",\n",
        "                        headers=self.headers,\n",
        "                        params=params\n",
        "                    )\n",
        "                    if response.status_code == 200:\n",
        "                        batch = response.json().get(\"data\", [])\n",
        "                        clips.extend(batch)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to fetch window {current_window_start}: {e}\")\n",
        "\n",
        "                # Move to next week\n",
        "                current_window_start = window_end\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Tiny sleep to be kind to the API rate limit\n",
        "                time.sleep(0.1)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Data Cleaning: Deduplication & Sorting\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Use a dictionary comprehension to remove duplicates based on Clip ID\n",
        "        unique_clips_map = {c['id']: c for c in clips}\n",
        "        unique_clips = list(unique_clips_map.values())\n",
        "\n",
        "        # Sort by view count (Highest to Lowest) to get the best content\n",
        "        sorted_clips = sorted(unique_clips, key=lambda x: x[\"view_count\"], reverse=True)\n",
        "\n",
        "        # Trim to exact target\n",
        "        final_list = sorted_clips[:Config.TOTAL_TARGET_CLIPS]\n",
        "\n",
        "        logger.info(f\"--- Metadata Done: Found {len(final_list)} unique top clips ---\")\n",
        "        return final_list\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 3. DOWNLOADING LOGIC (Static Method for Threading)\n",
        "    # --------------------------------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _download_worker(args: Tuple) -> Tuple[bool, Dict]:\n",
        "        \"\"\"\n",
        "        Worker function for ThreadPoolExecutor.\n",
        "        Downloads audio for a single clip using yt-dlp.\n",
        "\n",
        "        Args:\n",
        "            args: Tuple containing (clip_metadata, save_directory)\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (Success_Boolean, Clip_Metadata_or_Error_Message)\n",
        "        \"\"\"\n",
        "        clip, save_dir = args\n",
        "        try:\n",
        "            # Create a filesystem-safe filename\n",
        "            # Remove special chars, keep alphanumeric, spaces, dashes\n",
        "            safe_title = \"\".join([c for c in clip['title'] if c.isalnum() or c in (' ','-','_')])\n",
        "            safe_title = safe_title.strip()[:40] # Truncate to 40 chars\n",
        "\n",
        "            filename_base = f\"V{clip['view_count']}_{safe_title}_{clip['id']}\"\n",
        "            out_path_template = os.path.join(save_dir, filename_base)\n",
        "\n",
        "            # Check if already exists (resume capability)\n",
        "            if os.path.exists(out_path_template + \".m4a\"):\n",
        "                return (True, clip)\n",
        "\n",
        "            # Configure yt-dlp for high-speed audio extraction\n",
        "            ydl_opts = {\n",
        "                'format': 'bestaudio/best',\n",
        "                'postprocessors': [{\n",
        "                    'key': 'FFmpegExtractAudio',\n",
        "                    'preferredcodec': 'm4a', # m4a is faster/smaller than mp3 for this\n",
        "                }],\n",
        "                'outtmpl': out_path_template,\n",
        "                'quiet': True,\n",
        "                'no_warnings': True,\n",
        "            }\n",
        "\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([clip['url']])\n",
        "\n",
        "            return (True, clip)\n",
        "\n",
        "        except Exception as e:\n",
        "            return (False, f\"Clip {clip.get('id', 'Unknown')} Failed: {str(e)}\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 4. BATCH PROCESSING PIPELINE\n",
        "    # --------------------------------------------------------------------------\n",
        "    def process_and_upload(self, all_clips: List[Dict]):\n",
        "        \"\"\"\n",
        "        Splits metadata into batches, downloads them, zips them,\n",
        "        uploads to Google Drive, and cleans up local storage.\n",
        "        \"\"\"\n",
        "        total_batches = (len(all_clips) // Config.BATCH_SIZE) + 1\n",
        "\n",
        "        logger.info(f\"\\n--- Starting Processing: {len(all_clips)} clips in {total_batches} batches ---\")\n",
        "        logger.info(f\"ðŸ“‚ Target Drive Folder: {Config.DRIVE_FOLDER}\")\n",
        "\n",
        "        for i in range(total_batches):\n",
        "            batch_start = i * Config.BATCH_SIZE\n",
        "            batch_end = min((i + 1) * Config.BATCH_SIZE, len(all_clips))\n",
        "\n",
        "            if batch_start >= batch_end:\n",
        "                break\n",
        "\n",
        "            batch_clips = all_clips[batch_start:batch_end]\n",
        "            batch_num = i + 1\n",
        "\n",
        "            # A. Prepare Batch Directory\n",
        "            batch_dir_name = f\"batch_{batch_num}\"\n",
        "            batch_dir_path = os.path.join(Config.BASE_DIR, batch_dir_name)\n",
        "\n",
        "            # Ensure clean slate\n",
        "            if os.path.exists(batch_dir_path):\n",
        "                shutil.rmtree(batch_dir_path)\n",
        "            os.makedirs(batch_dir_path)\n",
        "\n",
        "            logger.info(f\"Processing Batch {batch_num}/{total_batches} (Clips {batch_start}-{batch_end})\")\n",
        "\n",
        "            # B. Parallel Download\n",
        "            download_args = [(clip, batch_dir_path) for clip in batch_clips]\n",
        "            successful_clips = []\n",
        "\n",
        "            # Use ThreadPoolExecutor for concurrent downloads\n",
        "            with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:\n",
        "                results = list(tqdm(\n",
        "                    executor.map(self._download_worker, download_args),\n",
        "                    total=len(batch_clips),\n",
        "                    unit=\"clip\",\n",
        "                    leave=False\n",
        "                ))\n",
        "\n",
        "            for success, data in results:\n",
        "                if success:\n",
        "                    successful_clips.append(data)\n",
        "                # Optional: Log failures here if needed\n",
        "\n",
        "            # C. Save Batch Metadata\n",
        "            # We save a JSON manifest so we know exactly what is in this zip\n",
        "            manifest = {c['id']: c for c in successful_clips}\n",
        "            with open(os.path.join(batch_dir_path, \"metadata.json\"), \"w\") as f:\n",
        "                json.dump(manifest, f, indent=4)\n",
        "\n",
        "            # D. Zip & Upload\n",
        "            zip_filename = f\"CS2_Batch_{batch_num}_{len(successful_clips)}clips\"\n",
        "            zip_path_no_ext = os.path.join(\"/content\", zip_filename) # make_archive adds .zip\n",
        "\n",
        "            shutil.make_archive(zip_path_no_ext, 'zip', batch_dir_path)\n",
        "\n",
        "            final_zip_path = zip_path_no_ext + \".zip\"\n",
        "            drive_dest_path = os.path.join(Config.DRIVE_FOLDER, f\"{zip_filename}.zip\")\n",
        "\n",
        "            logger.info(f\"â¬†ï¸ Uploading Batch {batch_num} to Drive...\")\n",
        "            shutil.move(final_zip_path, drive_dest_path)\n",
        "\n",
        "            # E. Cleanup\n",
        "            # Remove the unzipped folder to save Colab disk space\n",
        "            shutil.rmtree(batch_dir_path)\n",
        "\n",
        "            logger.info(f\"âœ… Batch {batch_num} Completed & Cleaned.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ENTRY POINT\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "\n",
        "    collector = TwitchAudioCollector()\n",
        "\n",
        "    try:\n",
        "        # 1. Login\n",
        "        collector.authenticate()\n",
        "\n",
        "        # 2. Get Data\n",
        "        all_clips_metadata = collector.fetch_historical_metadata()\n",
        "\n",
        "        # 3. Download & Upload\n",
        "        if all_clips_metadata:\n",
        "            collector.process_and_upload(all_clips_metadata)\n",
        "        else:\n",
        "            logger.warning(\"No clips found to download.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"\\nðŸ›‘ Process interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical Error: {e}\")\n",
        "    finally:\n",
        "        duration = (time.time() - start_time) / 60\n",
        "        logger.info(f\"--- Script Finished in {duration:.2f} minutes ---\")"
      ],
      "metadata": {
        "id": "Y7Q9qfuF2O6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNIzsLb33TFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading negative clips (i had downloaded Valorant seperately and CS2 and APex seperately, for testing purposes), this is the code for downloading CS2 and Apex Files"
      ],
      "metadata": {
        "id": "th8MGDuh3oX_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yDk1shb3qdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import subprocess\n",
        "import yt_dlp\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ================= SETTINGS =================\n",
        "# My Twitch Keys (don't share these!)\n",
        "CLIENT_ID = '4ursdzwlj6dbb35hxij9cnooap044r'\n",
        "CLIENT_SECRET = '4g8dimywmzmzw1e2aja3uabagvkxv2'\n",
        "\n",
        "# How many clips do we want?\n",
        "TOTAL_GOAL = 32000\n",
        "CLIPS_PER_VID = 3\n",
        "\n",
        "# Batch settings\n",
        "BATCH_SIZE = 350\n",
        "WORKERS = 8  # Number of threads to run at once\n",
        "CLIP_LENGTH = 30 # seconds\n",
        "\n",
        "# Folders\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Negative_Dataset_48k\"\n",
        "TEMP_FOLDER = \"/content/temp_mining_zone\"\n",
        "\n",
        "# Create the main folders if they don't exist\n",
        "if not os.path.exists(os.path.join(DRIVE_PATH, \"CS2_Set\")):\n",
        "    os.makedirs(os.path.join(DRIVE_PATH, \"CS2_Set\"))\n",
        "\n",
        "if not os.path.exists(os.path.join(DRIVE_PATH, \"Apex_Set\")):\n",
        "    os.makedirs(os.path.join(DRIVE_PATH, \"Apex_Set\"))\n",
        "\n",
        "# File that has all the links we need to download\n",
        "INPUT_FILE = os.path.join(DRIVE_PATH, \"download_manifest_split.json\")\n",
        "\n",
        "# This set will hold IDs of videos that failed so we don't try them again in other threads\n",
        "BROKEN_LINKS = set()\n",
        "\n",
        "# ================= HELPER FUNCTIONS =================\n",
        "\n",
        "def get_access_token():\n",
        "    # Need this to talk to Twitch API\n",
        "    url = f\"https://id.twitch.tv/oauth2/token?client_id={CLIENT_ID}&client_secret={CLIENT_SECRET}&grant_type=client_credentials\"\n",
        "    try:\n",
        "        resp = requests.post(url).json()\n",
        "        return {\"Client-ID\": CLIENT_ID, \"Authorization\": f\"Bearer {resp['access_token']}\"}\n",
        "    except:\n",
        "        print(\"Error getting token\")\n",
        "        return None\n",
        "\n",
        "def load_the_list():\n",
        "    # Reads the big json file with all the download tasks\n",
        "    if os.path.exists(INPUT_FILE):\n",
        "        with open(INPUT_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        print(\"Could not find the manifest file!\")\n",
        "        return []\n",
        "\n",
        "# ================= THE MAIN WORKER =================\n",
        "\n",
        "def process_one_video(task, save_folder):\n",
        "    # Check if this video is already known to be broken\n",
        "    if task['vod_id'] in BROKEN_LINKS:\n",
        "        return []\n",
        "\n",
        "    # File names\n",
        "    raw_name = f\"raw_{task['filename']}\"\n",
        "    raw_full_path = os.path.join(save_folder, raw_name + \".m4a\")\n",
        "    final_name_base = os.path.join(save_folder, task['filename'])\n",
        "\n",
        "    # List to store info about clips we make\n",
        "    clips_info = []\n",
        "\n",
        "    # Settings for the downloader (yt-dlp)\n",
        "    # I found these settings online to make it faster\n",
        "    dl_options = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{'key': 'FFmpegExtractAudio','preferredcodec': 'm4a'}],\n",
        "        'postprocessor_args': ['-t', '90'], # Download 90 seconds\n",
        "        # This lambda part tells it specifically where to start downloading\n",
        "        'download_ranges': lambda info, ydl: [{'start_time': task['start'], 'end_time': task['end']}],\n",
        "        'outtmpl': f\"{os.path.join(save_folder, raw_name)}.%(ext)s\",\n",
        "        'quiet': True,\n",
        "        'no_warnings': True,\n",
        "        'ignoreerrors': True,\n",
        "        'socket_timeout': 10,\n",
        "        'retries': 2\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Try to download\n",
        "        with yt_dlp.YoutubeDL(dl_options) as ydl:\n",
        "            ydl.download([task['vod_url']])\n",
        "\n",
        "        # Check if the file actually downloaded\n",
        "        if os.path.exists(raw_full_path) and os.path.getsize(raw_full_path) > 1000:\n",
        "\n",
        "            # If it worked, split it into 3 smaller parts using ffmpeg\n",
        "            for i in range(CLIPS_PER_VID):\n",
        "                part_filename = f\"{final_name_base}_part{i+1}.m4a\"\n",
        "                start_time = i * CLIP_LENGTH\n",
        "\n",
        "                # FFMPEG command to chop the audio\n",
        "                command = [\n",
        "                    'ffmpeg', '-y', '-v', 'error', '-i', raw_full_path,\n",
        "                    '-ss', str(start_time), '-t', str(CLIP_LENGTH), '-c', 'copy', part_filename\n",
        "                ]\n",
        "\n",
        "                try:\n",
        "                    subprocess.run(command, timeout=20)\n",
        "\n",
        "                    # If split worked, save the info\n",
        "                    if os.path.exists(part_filename):\n",
        "                        clips_info.append({\n",
        "                            \"filename\": os.path.basename(part_filename),\n",
        "                            \"label\": 0, # 0 means no exciting event\n",
        "                            \"source_vod\": task['vod_id'],\n",
        "                            \"timestamp\": task['start'] + start_time\n",
        "                        })\n",
        "                except:\n",
        "                    pass # Just skip if one chop fails\n",
        "\n",
        "            # Delete the big raw file to save space\n",
        "            os.remove(raw_full_path)\n",
        "            return clips_info\n",
        "\n",
        "        else:\n",
        "            # If download failed, add to bad list\n",
        "            BROKEN_LINKS.add(task['vod_id'])\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        # If anything else crashes, just mark it as broken\n",
        "        BROKEN_LINKS.add(task['vod_id'])\n",
        "        return []\n",
        "\n",
        "def run_a_batch(batch_number, tasks_in_batch):\n",
        "    # Figure out which game we are doing\n",
        "    game_name = tasks_in_batch[0]['game']\n",
        "\n",
        "    # Decide where to save the zip file\n",
        "    if game_name == \"CS2\":\n",
        "        dest_folder = os.path.join(DRIVE_PATH, \"CS2_Set\")\n",
        "        zip_prefix = \"cs2_batch\"\n",
        "    else:\n",
        "        dest_folder = os.path.join(DRIVE_PATH, \"Apex_Set\")\n",
        "        zip_prefix = \"apex_batch\"\n",
        "\n",
        "    print(f\"\\nStarting Batch {batch_number} for {game_name}...\")\n",
        "\n",
        "    # Create a temp folder for this batch\n",
        "    current_batch_folder = os.path.join(TEMP_FOLDER, f\"batch_{batch_number}\")\n",
        "    if os.path.exists(current_batch_folder):\n",
        "        shutil.rmtree(current_batch_folder)\n",
        "    os.makedirs(current_batch_folder)\n",
        "\n",
        "    all_metadata = []\n",
        "\n",
        "    # Run the downloads in parallel threads\n",
        "    with ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
        "        # Submit all tasks\n",
        "        futures = [executor.submit(process_one_video, t, current_batch_folder) for t in tasks_in_batch]\n",
        "\n",
        "        # Show progress bar\n",
        "        for f in tqdm(as_completed(futures), total=len(tasks_in_batch), desc=f\"Batch {batch_number}\", leave=False):\n",
        "            result = f.result()\n",
        "            if result:\n",
        "                all_metadata.extend(result)\n",
        "\n",
        "    # Save the metadata json for this batch\n",
        "    with open(os.path.join(current_batch_folder, \"metadata.json\"), 'w') as f:\n",
        "        json.dump(all_metadata, f)\n",
        "\n",
        "    # Zip everything up\n",
        "    zip_filename = f\"{zip_prefix}_{batch_number}.zip\"\n",
        "    zip_path = os.path.join(TEMP_FOLDER, zip_filename)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(current_batch_folder):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), file)\n",
        "\n",
        "    # Upload to Google Drive\n",
        "    print(f\"Uploading {zip_filename} to Drive...\", flush=True)\n",
        "    shutil.copy2(zip_path, os.path.join(dest_folder, zip_filename))\n",
        "\n",
        "    # Cleanup\n",
        "    shutil.rmtree(current_batch_folder)\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Batch {batch_number} finished. Broken VODs ignored: {len(BROKEN_LINKS)}\")\n",
        "\n",
        "# ================= START HERE =================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Load the big list of things to do\n",
        "    full_list = load_the_list()\n",
        "\n",
        "    # Split list by game\n",
        "    cs2_list = [x for x in full_list if x['game'] == 'CS2']\n",
        "    apex_list = [x for x in full_list if x['game'] == 'Apex']\n",
        "\n",
        "    # --- DO CS2 BATCHES ---\n",
        "    print(f\"\\nDoing CS2 tasks ({len(cs2_list)} items)...\")\n",
        "\n",
        "    # Calculate how many batches we need\n",
        "    num_batches_cs2 = (len(cs2_list) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches_cs2):\n",
        "        # Check if we already did this one\n",
        "        zip_check = f\"cs2_batch_{i}.zip\"\n",
        "        path_check = os.path.join(DRIVE_PATH, \"CS2_Set\", zip_check)\n",
        "\n",
        "        if os.path.exists(path_check) and os.path.getsize(path_check) > 1000:\n",
        "            print(f\"Skipping Batch {i} (Already done)\")\n",
        "            continue\n",
        "\n",
        "        # Get the slice of tasks for this batch\n",
        "        start_index = i * BATCH_SIZE\n",
        "        end_index = min((i + 1) * BATCH_SIZE, len(cs2_list))\n",
        "        batch_tasks = cs2_list[start_index:end_index]\n",
        "\n",
        "        run_a_batch(i, batch_tasks)\n",
        "\n",
        "    # --- DO APEX BATCHES ---\n",
        "    print(f\"\\nDoing Apex tasks ({len(apex_list)} items)...\")\n",
        "\n",
        "    num_batches_apex = (len(apex_list) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches_apex):\n",
        "        zip_check = f\"apex_batch_{i}.zip\"\n",
        "        path_check = os.path.join(DRIVE_PATH, \"Apex_Set\", zip_check)\n",
        "\n",
        "        if os.path.exists(path_check) and os.path.getsize(path_check) > 1000:\n",
        "            print(f\"Skipping Batch {i} (Already done)\")\n",
        "            continue\n",
        "\n",
        "        start_index = i * BATCH_SIZE\n",
        "        end_index = min((i + 1) * BATCH_SIZE, len(apex_list))\n",
        "        batch_tasks = apex_list[start_index:end_index]\n",
        "\n",
        "        run_a_batch(i, batch_tasks)\n",
        "\n",
        "    print(\"All done!\")"
      ],
      "metadata": {
        "id": "n5MZg1vqZJmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jzgFO6U4Pun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading all this data, we have to run it through Demucs,"
      ],
      "metadata": {
        "id": "c0Yyj0_L4Sbj"
      }
    }
  ]
}