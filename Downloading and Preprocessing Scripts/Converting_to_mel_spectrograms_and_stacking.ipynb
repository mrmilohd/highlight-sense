{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Description of this code, this code is AI Generated**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is the final step in data processing, this code reads the .mp3 files from demcus conversion and uses librosa to convert all the 66k* clips to log mel spectrograms, and stack its game and streamer audio channels together. For speeding up the concept of batch input and output is present, with checkpoints, so that the script can resume if colab closes unexpectedly in between"
      ],
      "metadata": {
        "id": "zm4eKkNsOoPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install librosa numpy torch tqdm\n",
        "print(\"‚úÖ Ready for Librosa!\")"
      ],
      "metadata": {
        "id": "8OofbiEiwa8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "import uuid\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "POSITIVE_INPUT = \"/content/drive/MyDrive/Separated_Audio\"\n",
        "NEGATIVE_INPUT = \"/content/drive/MyDrive/separated_audio_negative\"\n",
        "OUTPUT_ROOT = \"/content/drive/MyDrive/Mel_Spectrograms\"\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 80\n",
        "\n",
        "SHARD_SIZE = 2048\n",
        "\n",
        "# Game folders mapping\n",
        "GAME_FOLDERS = {\n",
        "    \"Valorant_Separated\": \"Valorant\",\n",
        "    \"CS2_Separated\": \"CS2\",\n",
        "    \"Apex_Separated\": \"Apex\"\n",
        "}\n",
        "\n",
        "# Local temp folders\n",
        "TEMP_EXTRACT = \"/content/temp_extract\"\n",
        "TEMP_BUFFER = \"/content/temp_buffer\"\n",
        "CHECKPOINT_FILE = None\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ================= SETUP =================\n",
        "def setup():\n",
        "    global CHECKPOINT_FILE\n",
        "\n",
        "    print(\"üöÄ Mel Spectrogram Converter\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create output folders\n",
        "    for label in [\"Positive\", \"Negative\"]:\n",
        "        for game in GAME_FOLDERS.values():\n",
        "            folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "            os.makedirs(folder, exist_ok=True)\n",
        "            print(f\"üìÅ {label}/{game}\")\n",
        "\n",
        "    # Temp folders\n",
        "    os.makedirs(TEMP_EXTRACT, exist_ok=True)\n",
        "    os.makedirs(TEMP_BUFFER, exist_ok=True)\n",
        "\n",
        "    CHECKPOINT_FILE = os.path.join(OUTPUT_ROOT, \"mel_checkpoint.json\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    return True\n",
        "\n",
        "# ================= CHECKPOINT =================\n",
        "def load_checkpoint():\n",
        "    if CHECKPOINT_FILE and os.path.exists(CHECKPOINT_FILE):\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'r') as f:\n",
        "                cp = json.load(f)\n",
        "            print(f\"üì• Checkpoint: {cp.get('total_processed', 0)} clips done\")\n",
        "            return cp\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return {\n",
        "        \"processed_zips\": [],\n",
        "        \"shard_counts\": {\n",
        "            \"Positive\": {\"Valorant\": 0, \"CS2\": 0, \"Apex\": 0},\n",
        "            \"Negative\": {\"Valorant\": 0, \"CS2\": 0, \"Apex\": 0}\n",
        "        },\n",
        "        \"total_processed\": 0,\n",
        "        \"total_errors\": 0\n",
        "    }\n",
        "\n",
        "def save_checkpoint(cp):\n",
        "    if CHECKPOINT_FILE:\n",
        "        cp[\"last_save\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(cp, f, indent=2)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# ================= AUDIO PROCESSING =================\n",
        "def audio_to_mel(audio_path):\n",
        "    \"\"\"Convert audio file to log mel spectrogram tensor.\"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "        if len(y) == 0:\n",
        "            return None\n",
        "\n",
        "        # Create mel spectrogram\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            n_fft=N_FFT,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_mels=N_MELS\n",
        "        )\n",
        "\n",
        "        # Convert to tensor and log scale\n",
        "        mel_tensor = torch.from_numpy(mel_spec).float()\n",
        "        log_mel = torch.log(mel_tensor + 1e-9)\n",
        "\n",
        "        # Shape: (1, n_mels, time)\n",
        "        return log_mel.unsqueeze(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def process_clip(clip_folder, temp_vocals, temp_no_vocals):\n",
        "    \"\"\"Process a clip folder containing vocals.mp3 and no_vocals.mp3.\n",
        "\n",
        "    Returns: dict with 'vocals' and 'no_vocals' tensors, or None if failed\n",
        "    \"\"\"\n",
        "    vocals_path = os.path.join(clip_folder, \"vocals.mp3\")\n",
        "    no_vocals_path = os.path.join(clip_folder, \"no_vocals.mp3\")\n",
        "\n",
        "    # Check both files exist\n",
        "    if not os.path.exists(vocals_path) or not os.path.exists(no_vocals_path):\n",
        "        return None\n",
        "\n",
        "    # Process both tracks\n",
        "    vocals_mel = audio_to_mel(vocals_path)\n",
        "    no_vocals_mel = audio_to_mel(no_vocals_path)\n",
        "\n",
        "    if vocals_mel is None or no_vocals_mel is None:\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"vocals\": vocals_mel,\n",
        "        \"no_vocals\": no_vocals_mel\n",
        "    }\n",
        "\n",
        "# ================= SHARDING =================\n",
        "def flush_buffer(buffer_path, output_folder, shard_id):\n",
        "    \"\"\"Zip all .pt files in buffer to a shard.\"\"\"\n",
        "    files = glob.glob(os.path.join(buffer_path, \"*.pt\"))\n",
        "    if not files:\n",
        "        return False\n",
        "\n",
        "    zip_name = f\"shard_{shard_id}_{len(files)}clips.zip\"\n",
        "    zip_path = os.path.join(output_folder, zip_name)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "            for f in files:\n",
        "                zf.write(f, os.path.basename(f))\n",
        "\n",
        "        # Cleanup buffer\n",
        "        for f in files:\n",
        "            os.remove(f)\n",
        "\n",
        "        size_mb = os.path.getsize(zip_path) / (1024**2)\n",
        "        tqdm.write(f\"   üì¶ Shard {shard_id}: {len(files)} clips ({size_mb:.1f} MB)\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"   ‚ùå Shard error: {e}\")\n",
        "        return False\n",
        "\n",
        "# ================= MAIN PROCESSING =================\n",
        "def process_dataset(input_folder, label, checkpoint):\n",
        "    \"\"\"Process all games in a dataset folder (Positive or Negative).\"\"\"\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"üéÆ Processing {label} Dataset\")\n",
        "    print(f\"üìÇ {input_folder}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(f\"‚ùå Folder not found: {input_folder}\")\n",
        "        return\n",
        "\n",
        "    total_clips = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    # Process each game folder\n",
        "    for game_folder, game_name in GAME_FOLDERS.items():\n",
        "        game_path = os.path.join(input_folder, game_folder)\n",
        "\n",
        "        if not os.path.exists(game_path):\n",
        "            print(f\"‚ö†Ô∏è {game_name}: folder not found, skipping\")\n",
        "            continue\n",
        "\n",
        "        # Find all batch zips\n",
        "        batch_zips = sorted(glob.glob(os.path.join(game_path, \"*.zip\")))\n",
        "\n",
        "        if not batch_zips:\n",
        "            print(f\"‚ö†Ô∏è {game_name}: no zip files found\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüéÆ {game_name}: {len(batch_zips)} batch zips\")\n",
        "\n",
        "        # Output folder for this game\n",
        "        output_folder = os.path.join(OUTPUT_ROOT, label, game_name)\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        # Buffer for this game\n",
        "        game_buffer = os.path.join(TEMP_BUFFER, f\"{label}_{game_name}\")\n",
        "        if os.path.exists(game_buffer):\n",
        "            shutil.rmtree(game_buffer)\n",
        "        os.makedirs(game_buffer)\n",
        "\n",
        "        buffer_count = 0\n",
        "        shard_id = checkpoint[\"shard_counts\"][label][game_name]\n",
        "        game_clips = 0\n",
        "        game_errors = 0\n",
        "\n",
        "        # Process each batch zip\n",
        "        for batch_zip in tqdm(batch_zips, desc=f\"[{game_name}]\"):\n",
        "            zip_key = f\"{label}_{game_name}_{os.path.basename(batch_zip)}\"\n",
        "\n",
        "            # Skip if already processed\n",
        "            if zip_key in checkpoint[\"processed_zips\"]:\n",
        "                continue\n",
        "\n",
        "            # Extract batch zip\n",
        "            extract_path = os.path.join(TEMP_EXTRACT, f\"{label}_{game_name}\")\n",
        "            if os.path.exists(extract_path):\n",
        "                shutil.rmtree(extract_path)\n",
        "            os.makedirs(extract_path)\n",
        "\n",
        "            try:\n",
        "                with zipfile.ZipFile(batch_zip, 'r') as zf:\n",
        "                    zf.extractall(extract_path)\n",
        "\n",
        "                # Find all clip folders (folders containing vocals.mp3)\n",
        "                clip_folders = []\n",
        "                for root, dirs, files in os.walk(extract_path):\n",
        "                    if \"vocals.mp3\" in files and \"no_vocals.mp3\" in files:\n",
        "                        clip_folders.append(root)\n",
        "\n",
        "                tqdm.write(f\"   üìÇ {os.path.basename(batch_zip)}: {len(clip_folders)} clips\")\n",
        "\n",
        "                # Process each clip\n",
        "                for clip_folder in clip_folders:\n",
        "                    clip_name = os.path.basename(clip_folder)\n",
        "\n",
        "                    result = process_clip(clip_folder, None, None)\n",
        "\n",
        "                    if result is not None:\n",
        "                        # Save as single .pt file with both tensors\n",
        "                        pt_filename = f\"{game_name}_{clip_name}_{uuid.uuid4().hex[:8]}.pt\"\n",
        "                        pt_path = os.path.join(game_buffer, pt_filename)\n",
        "\n",
        "                        # Save dict with both spectrograms\n",
        "                        torch.save({\n",
        "                            \"vocals\": result[\"vocals\"],\n",
        "                            \"no_vocals\": result[\"no_vocals\"],\n",
        "                            \"clip_name\": clip_name,\n",
        "                            \"game\": game_name,\n",
        "                            \"label\": label\n",
        "                        }, pt_path)\n",
        "\n",
        "                        buffer_count += 1\n",
        "                        game_clips += 1\n",
        "\n",
        "                        # Flush buffer if full\n",
        "                        if buffer_count >= SHARD_SIZE:\n",
        "                            flush_buffer(game_buffer, output_folder, shard_id)\n",
        "                            shard_id += 1\n",
        "                            checkpoint[\"shard_counts\"][label][game_name] = shard_id\n",
        "                            buffer_count = 0\n",
        "                            save_checkpoint(checkpoint)\n",
        "                    else:\n",
        "                        game_errors += 1\n",
        "\n",
        "                # Mark zip as processed\n",
        "                checkpoint[\"processed_zips\"].append(zip_key)\n",
        "                save_checkpoint(checkpoint)\n",
        "\n",
        "            except zipfile.BadZipFile:\n",
        "                tqdm.write(f\"   ‚ùå Corrupted zip: {os.path.basename(batch_zip)}\")\n",
        "                game_errors += 1\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"   ‚ùå Error: {e}\")\n",
        "                game_errors += 1\n",
        "            finally:\n",
        "                # Cleanup extract folder\n",
        "                if os.path.exists(extract_path):\n",
        "                    shutil.rmtree(extract_path, ignore_errors=True)\n",
        "\n",
        "        # Flush remaining buffer\n",
        "        if buffer_count > 0:\n",
        "            flush_buffer(game_buffer, output_folder, shard_id)\n",
        "            shard_id += 1\n",
        "            checkpoint[\"shard_counts\"][label][game_name] = shard_id\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "        # Cleanup game buffer\n",
        "        if os.path.exists(game_buffer):\n",
        "            shutil.rmtree(game_buffer, ignore_errors=True)\n",
        "\n",
        "        print(f\"   ‚úÖ {game_name}: {game_clips} clips, {shard_id} shards\" +\n",
        "              (f\", {game_errors} errors\" if game_errors else \"\"))\n",
        "\n",
        "        total_clips += game_clips\n",
        "        total_errors += game_errors\n",
        "\n",
        "    checkpoint[\"total_processed\"] += total_clips\n",
        "    checkpoint[\"total_errors\"] += total_errors\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    print(f\"\\n‚úÖ {label} Complete: {total_clips} clips processed\")\n",
        "\n",
        "# ================= MAIN =================\n",
        "def run_pipeline():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéµ MEL SPECTROGRAM CONVERTER\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üéµ Sample Rate: {SAMPLE_RATE} Hz\")\n",
        "    print(f\"üéµ Mel Bins: {N_MELS}\")\n",
        "    print(f\"üì¶ Shard Size: {SHARD_SIZE} clips\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not setup():\n",
        "        return\n",
        "\n",
        "    checkpoint = load_checkpoint()\n",
        "\n",
        "    # Process Positive dataset\n",
        "    process_dataset(POSITIVE_INPUT, \"Positive\", checkpoint)\n",
        "\n",
        "    # Process Negative dataset\n",
        "    process_dataset(NEGATIVE_INPUT, \"Negative\", checkpoint)\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(TEMP_EXTRACT):\n",
        "        shutil.rmtree(TEMP_EXTRACT, ignore_errors=True)\n",
        "    if os.path.exists(TEMP_BUFFER):\n",
        "        shutil.rmtree(TEMP_BUFFER, ignore_errors=True)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚úÖ Total Clips: {checkpoint['total_processed']}\")\n",
        "    print(f\"‚ùå Total Errors: {checkpoint['total_errors']}\")\n",
        "    print(f\"‚è∞ Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    print(\"\\nüìä Shard Summary:\")\n",
        "    for label in [\"Positive\", \"Negative\"]:\n",
        "        print(f\"\\n   {label}:\")\n",
        "        for game in GAME_FOLDERS.values():\n",
        "            count = checkpoint[\"shard_counts\"][label][game]\n",
        "            folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "            if os.path.exists(folder):\n",
        "                zips = glob.glob(os.path.join(folder, \"*.zip\"))\n",
        "                size_mb = sum(os.path.getsize(z) for z in zips) / (1024**2)\n",
        "                print(f\"      {game}: {count} shards ({size_mb:.1f} MB)\")\n",
        "\n",
        "# ================= RUN =================\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "V7la0aZ6zmeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "\n",
        "INPUT_ROOT = \"/content/drive/MyDrive/Mel_Spectrograms\"\n",
        "OUTPUT_ROOT = \"/content/drive/MyDrive/Stacked_Tensors\"\n",
        "\n",
        "SHARD_SIZE = 2048\n",
        "\n",
        "# Structure\n",
        "LABELS = [\"Positive\", \"Negative\"]\n",
        "GAMES = [\"Valorant\", \"CS2\", \"Apex\"]\n",
        "\n",
        "# Temp folders\n",
        "TEMP_EXTRACT = \"/content/temp_stack_extract\"\n",
        "TEMP_BUFFER = \"/content/temp_stack_buffer\"\n",
        "CHECKPOINT_FILE = None\n",
        "\n",
        "# ================= SETUP =================\n",
        "def setup():\n",
        "    global CHECKPOINT_FILE\n",
        "\n",
        "    print(\"üîó Tensor Stacker\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create output folders\n",
        "    for label in LABELS:\n",
        "        for game in GAMES:\n",
        "            folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "            os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    os.makedirs(TEMP_EXTRACT, exist_ok=True)\n",
        "    os.makedirs(TEMP_BUFFER, exist_ok=True)\n",
        "\n",
        "    CHECKPOINT_FILE = os.path.join(OUTPUT_ROOT, \"stack_checkpoint.json\")\n",
        "\n",
        "    print(f\"üìÇ Input: {INPUT_ROOT}\")\n",
        "    print(f\"üìÇ Output: {OUTPUT_ROOT}\")\n",
        "    print(\"=\" * 60)\n",
        "    return True\n",
        "\n",
        "# ================= CHECKPOINT =================\n",
        "def load_checkpoint():\n",
        "    if CHECKPOINT_FILE and os.path.exists(CHECKPOINT_FILE):\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return {\n",
        "        \"processed_shards\": [],\n",
        "        \"shard_counts\": {label: {game: 0 for game in GAMES} for label in LABELS},\n",
        "        \"total_processed\": 0\n",
        "    }\n",
        "\n",
        "def save_checkpoint(cp):\n",
        "    if CHECKPOINT_FILE:\n",
        "        cp[\"last_save\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        with open(CHECKPOINT_FILE, 'w') as f:\n",
        "            json.dump(cp, f, indent=2)\n",
        "\n",
        "# ================= STACKING =================\n",
        "def stack_tensor(data):\n",
        "    \"\"\"Stack vocals and no_vocals into single tensor (2, 80, time).\"\"\"\n",
        "    vocals = data[\"vocals\"]      # (1, 80, time)\n",
        "    no_vocals = data[\"no_vocals\"]  # (1, 80, time)\n",
        "\n",
        "    # Handle different lengths - pad shorter to match longer\n",
        "    v_time = vocals.shape[-1]\n",
        "    nv_time = no_vocals.shape[-1]\n",
        "\n",
        "    if v_time != nv_time:\n",
        "        max_time = max(v_time, nv_time)\n",
        "        if v_time < max_time:\n",
        "            pad = torch.zeros(1, vocals.shape[1], max_time - v_time)\n",
        "            vocals = torch.cat([vocals, pad], dim=-1)\n",
        "        if nv_time < max_time:\n",
        "            pad = torch.zeros(1, no_vocals.shape[1], max_time - nv_time)\n",
        "            no_vocals = torch.cat([no_vocals, pad], dim=-1)\n",
        "\n",
        "    # Stack: (2, 80, time)\n",
        "    stacked = torch.cat([vocals, no_vocals], dim=0)\n",
        "\n",
        "    return stacked\n",
        "\n",
        "def flush_buffer(buffer_path, output_folder, shard_id):\n",
        "    \"\"\"Zip all .pt files in buffer.\"\"\"\n",
        "    files = glob.glob(os.path.join(buffer_path, \"*.pt\"))\n",
        "    if not files:\n",
        "        return False\n",
        "\n",
        "    zip_name = f\"stacked_shard_{shard_id}_{len(files)}clips.zip\"\n",
        "    zip_path = os.path.join(output_folder, zip_name)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        for f in files:\n",
        "            zf.write(f, os.path.basename(f))\n",
        "\n",
        "    for f in files:\n",
        "        os.remove(f)\n",
        "\n",
        "    size_mb = os.path.getsize(zip_path) / (1024**2)\n",
        "    tqdm.write(f\"   üì¶ Shard {shard_id}: {len(files)} clips ({size_mb:.1f} MB)\")\n",
        "    return True\n",
        "\n",
        "# ================= MAIN PROCESSING =================\n",
        "def process_all():\n",
        "    checkpoint = load_checkpoint()\n",
        "    total = 0\n",
        "\n",
        "    for label in LABELS:\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"üìÅ {label}\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "\n",
        "        for game in GAMES:\n",
        "            input_folder = os.path.join(INPUT_ROOT, label, game)\n",
        "            output_folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "\n",
        "            if not os.path.exists(input_folder):\n",
        "                print(f\"‚ö†Ô∏è {game}: not found, skipping\")\n",
        "                continue\n",
        "\n",
        "            # Find input shards\n",
        "            input_shards = sorted(glob.glob(os.path.join(input_folder, \"*.zip\")))\n",
        "\n",
        "            if not input_shards:\n",
        "                print(f\"‚ö†Ô∏è {game}: no shards found\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nüéÆ {game}: {len(input_shards)} input shards\")\n",
        "\n",
        "            # Buffer for this game\n",
        "            game_buffer = os.path.join(TEMP_BUFFER, f\"{label}_{game}\")\n",
        "            if os.path.exists(game_buffer):\n",
        "                shutil.rmtree(game_buffer)\n",
        "            os.makedirs(game_buffer)\n",
        "\n",
        "            buffer_count = 0\n",
        "            shard_id = checkpoint[\"shard_counts\"][label][game]\n",
        "            game_total = 0\n",
        "\n",
        "            for shard_zip in tqdm(input_shards, desc=f\"[{game}]\"):\n",
        "                shard_key = f\"{label}_{game}_{os.path.basename(shard_zip)}\"\n",
        "\n",
        "                if shard_key in checkpoint[\"processed_shards\"]:\n",
        "                    continue\n",
        "\n",
        "                # Extract shard\n",
        "                extract_path = os.path.join(TEMP_EXTRACT, f\"{label}_{game}\")\n",
        "                if os.path.exists(extract_path):\n",
        "                    shutil.rmtree(extract_path)\n",
        "                os.makedirs(extract_path)\n",
        "\n",
        "                try:\n",
        "                    with zipfile.ZipFile(shard_zip, 'r') as zf:\n",
        "                        zf.extractall(extract_path)\n",
        "\n",
        "                    # Process each .pt file\n",
        "                    pt_files = glob.glob(os.path.join(extract_path, \"*.pt\"))\n",
        "\n",
        "                    for pt_file in pt_files:\n",
        "                        try:\n",
        "                            data = torch.load(pt_file, weights_only=False)\n",
        "\n",
        "                            # Stack tensors\n",
        "                            stacked = stack_tensor(data)\n",
        "\n",
        "                            # Save stacked tensor\n",
        "                            out_name = os.path.basename(pt_file).replace(\".pt\", \"_stacked.pt\")\n",
        "                            out_path = os.path.join(game_buffer, out_name)\n",
        "\n",
        "                            torch.save({\n",
        "                                \"tensor\": stacked,  # (2, 80, time)\n",
        "                                \"clip_name\": data.get(\"clip_name\", \"\"),\n",
        "                                \"game\": game,\n",
        "                                \"label\": label\n",
        "                            }, out_path)\n",
        "\n",
        "                            buffer_count += 1\n",
        "                            game_total += 1\n",
        "\n",
        "                            # Flush if buffer full\n",
        "                            if buffer_count >= SHARD_SIZE:\n",
        "                                flush_buffer(game_buffer, output_folder, shard_id)\n",
        "                                shard_id += 1\n",
        "                                checkpoint[\"shard_counts\"][label][game] = shard_id\n",
        "                                buffer_count = 0\n",
        "                                save_checkpoint(checkpoint)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                    checkpoint[\"processed_shards\"].append(shard_key)\n",
        "                    save_checkpoint(checkpoint)\n",
        "\n",
        "                except Exception as e:\n",
        "                    tqdm.write(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "                finally:\n",
        "                    if os.path.exists(extract_path):\n",
        "                        shutil.rmtree(extract_path, ignore_errors=True)\n",
        "\n",
        "            # Flush remaining\n",
        "            if buffer_count > 0:\n",
        "                flush_buffer(game_buffer, output_folder, shard_id)\n",
        "                shard_id += 1\n",
        "                checkpoint[\"shard_counts\"][label][game] = shard_id\n",
        "                save_checkpoint(checkpoint)\n",
        "\n",
        "            # Cleanup\n",
        "            if os.path.exists(game_buffer):\n",
        "                shutil.rmtree(game_buffer, ignore_errors=True)\n",
        "\n",
        "            print(f\"   ‚úÖ {game}: {game_total} clips stacked, {shard_id} shards\")\n",
        "            total += game_total\n",
        "\n",
        "    checkpoint[\"total_processed\"] = total\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    return total\n",
        "\n",
        "# ================= RUN =================\n",
        "def run_pipeline():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üîó TENSOR STACKER\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üìê Output shape: (2, 80, time)\")\n",
        "    print(f\"   Channel 0: vocals\")\n",
        "    print(f\"   Channel 1: no_vocals\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    setup()\n",
        "    total = process_all()\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(TEMP_EXTRACT):\n",
        "        shutil.rmtree(TEMP_EXTRACT, ignore_errors=True)\n",
        "    if os.path.exists(TEMP_BUFFER):\n",
        "        shutil.rmtree(TEMP_BUFFER, ignore_errors=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚úÖ Total stacked: {total} clips\")\n",
        "    print(f\"‚è∞ Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\nüìä Output Summary:\")\n",
        "    for label in LABELS:\n",
        "        print(f\"\\n   {label}:\")\n",
        "        for game in GAMES:\n",
        "            folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "            if os.path.exists(folder):\n",
        "                zips = glob.glob(os.path.join(folder, \"*.zip\"))\n",
        "                if zips:\n",
        "                    size_mb = sum(os.path.getsize(z) for z in zips) / (1024**2)\n",
        "                    print(f\"      {game}: {len(zips)} shards ({size_mb:.1f} MB)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "7H0qenQV3cwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FAST VERSION - Process locally, not on Drive\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import torch\n",
        "import io\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "INPUT_ROOT = \"/content/drive/MyDrive/Mel_Spectrograms\"\n",
        "OUTPUT_ROOT = \"/content/drive/MyDrive/Stacked_Tensors\"\n",
        "LOCAL_INPUT = \"/content/local_input\"\n",
        "LOCAL_OUTPUT = \"/content/local_output\"\n",
        "SHARD_SIZE = 2048\n",
        "\n",
        "LABELS = [\"Positive\", \"Negative\"]\n",
        "GAMES = [\"Valorant\", \"CS2\", \"Apex\"]\n",
        "\n",
        "print(\"üîó ULTRA-FAST TENSOR STACKER (Local Processing)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "total = 0\n",
        "\n",
        "for label in LABELS:\n",
        "    print(f\"\\nüìÅ {label}\")\n",
        "\n",
        "    for game in GAMES:\n",
        "        input_folder = os.path.join(INPUT_ROOT, label, game)\n",
        "        output_folder = os.path.join(OUTPUT_ROOT, label, game)\n",
        "        local_in = os.path.join(LOCAL_INPUT, label, game)\n",
        "        local_out = os.path.join(LOCAL_OUTPUT, label, game)\n",
        "\n",
        "        if not os.path.exists(input_folder):\n",
        "            continue\n",
        "\n",
        "        input_zips = sorted(glob.glob(os.path.join(input_folder, \"*.zip\")))\n",
        "        if not input_zips:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n   üéÆ {game}: {len(input_zips)} shards\")\n",
        "\n",
        "        # Create local folders\n",
        "        os.makedirs(local_in, exist_ok=True)\n",
        "        os.makedirs(local_out, exist_ok=True)\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        all_stacked = []\n",
        "        shard_id = 0\n",
        "\n",
        "        for zip_path in tqdm(input_zips, desc=f\"   [{game}]\"):\n",
        "            # Copy to local\n",
        "            local_zip = os.path.join(local_in, os.path.basename(zip_path))\n",
        "            shutil.copy(zip_path, local_zip)\n",
        "\n",
        "            with zipfile.ZipFile(local_zip, 'r') as zf:\n",
        "                pt_files = [n for n in zf.namelist() if n.endswith('.pt')]\n",
        "\n",
        "                for pt_name in pt_files:\n",
        "                    try:\n",
        "                        with zf.open(pt_name) as f:\n",
        "                            data = torch.load(io.BytesIO(f.read()), weights_only=False)\n",
        "\n",
        "                        vocals = data[\"vocals\"]\n",
        "                        no_vocals = data[\"no_vocals\"]\n",
        "\n",
        "                        # Pad if needed\n",
        "                        v_t, nv_t = vocals.shape[-1], no_vocals.shape[-1]\n",
        "                        if v_t != nv_t:\n",
        "                            max_t = max(v_t, nv_t)\n",
        "                            if v_t < max_t:\n",
        "                                vocals = torch.cat([vocals, torch.zeros(1, 80, max_t - v_t)], dim=-1)\n",
        "                            if nv_t < max_t:\n",
        "                                no_vocals = torch.cat([no_vocals, torch.zeros(1, 80, max_t - nv_t)], dim=-1)\n",
        "\n",
        "                        stacked = torch.cat([vocals, no_vocals], dim=0)\n",
        "\n",
        "                        all_stacked.append({\n",
        "                            \"tensor\": stacked,\n",
        "                            \"clip_name\": data.get(\"clip_name\", \"\"),\n",
        "                            \"game\": game,\n",
        "                            \"label\": label\n",
        "                        })\n",
        "\n",
        "                        # Flush shard locally, then copy to Drive\n",
        "                        if len(all_stacked) >= SHARD_SIZE:\n",
        "                            local_shard = os.path.join(local_out, f\"stacked_shard_{shard_id}.zip\")\n",
        "                            with zipfile.ZipFile(local_shard, 'w', zipfile.ZIP_DEFLATED) as ozf:\n",
        "                                for i, item in enumerate(all_stacked):\n",
        "                                    pt_bytes = io.BytesIO()\n",
        "                                    torch.save(item, pt_bytes)\n",
        "                                    ozf.writestr(f\"clip_{shard_id}_{i}.pt\", pt_bytes.getvalue())\n",
        "\n",
        "                            # Copy to Drive\n",
        "                            shutil.copy(local_shard, os.path.join(output_folder, f\"stacked_shard_{shard_id}_{len(all_stacked)}clips.zip\"))\n",
        "                            os.remove(local_shard)\n",
        "\n",
        "                            tqdm.write(f\"      üì¶ Shard {shard_id}: {len(all_stacked)} clips\")\n",
        "                            total += len(all_stacked)\n",
        "                            all_stacked = []\n",
        "                            shard_id += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Cleanup local zip\n",
        "            os.remove(local_zip)\n",
        "\n",
        "        # Flush remaining\n",
        "        if all_stacked:\n",
        "            local_shard = os.path.join(local_out, f\"stacked_shard_{shard_id}.zip\")\n",
        "            with zipfile.ZipFile(local_shard, 'w', zipfile.ZIP_DEFLATED) as ozf:\n",
        "                for i, item in enumerate(all_stacked):\n",
        "                    pt_bytes = io.BytesIO()\n",
        "                    torch.save(item, pt_bytes)\n",
        "                    ozf.writestr(f\"clip_{shard_id}_{i}.pt\", pt_bytes.getvalue())\n",
        "\n",
        "            shutil.copy(local_shard, os.path.join(output_folder, f\"stacked_shard_{shard_id}_{len(all_stacked)}clips.zip\"))\n",
        "            tqdm.write(f\"      üì¶ Shard {shard_id}: {len(all_stacked)} clips\")\n",
        "            total += len(all_stacked)\n",
        "\n",
        "# Cleanup\n",
        "shutil.rmtree(LOCAL_INPUT, ignore_errors=True)\n",
        "shutil.rmtree(LOCAL_OUTPUT, ignore_errors=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Done! Total: {total} clips stacked\")"
      ],
      "metadata": {
        "id": "6ebINV5x4ZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7VjKZoXCkJZ"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "from google.colab import drive\n",
        "\n",
        "gc.collect() # Clean up memory and potentially release file handles\n",
        "\n",
        "# Check if Drive is already mounted and unmount if necessary\n",
        "if os.path.ismount('/content/drive'):\n",
        "    print(\"Google Drive is currently mounted. Attempting to unmount...\")\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "        print(\"Google Drive unmounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error during unmount: {e}. Proceeding with directory cleanup.\")\n",
        "\n",
        "# Ensure the mount point directory is absolutely empty\n",
        "if os.path.exists('/content/drive'):\n",
        "    print(\"'/content/drive' directory exists. Clearing its contents...\")\n",
        "    # Forcefully remove all contents, including hidden files/dirs\n",
        "    !rm -rf /content/drive/*\n",
        "    # Try to remove the directory itself if it's empty, ignore errors\n",
        "    !rmdir /content/drive || true\n",
        "\n",
        "# Recreate a fresh, empty mount point directory\n",
        "os.makedirs('/content/drive', exist_ok=True)\n",
        "print(\"'/content/drive' directory prepared.\")\n",
        "\n",
        "# Verify it's empty before attempting to mount\n",
        "if os.listdir('/content/drive'):\n",
        "    print(f\"Critical Error: '/content/drive' is NOT empty before mount: {os.listdir('/content/drive')}\")\n",
        "else:\n",
        "    print(\"'/content/drive' is verified empty before mounting.\")\n",
        "\n",
        "# Attempt to mount Google Drive\n",
        "print(\"Attempting to mount Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}