{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEoP7LtMGorR"
      },
      "source": [
        "**Inference Pipeline**\n",
        "---\n",
        "**Important notes, and requirements**\n",
        "*   Make sure the enviroment is running on google colab.\n",
        "*   Download the best_model.pt file from github, and upload it directly onto the local folder in colab runtime, hence the final path of the model being /content/best_model.pt\n",
        "* Change the URL  at twitch_url, by copy pasting the link of the twitch VOD you want to generate highlights for and pasting the link inside the double quotes\n",
        "* This pipeline uses Demucs, an audio seperation tool, which requires a decent GPU- cuda V12.5.8, the tested inference was done using T4 gpu of colab\n",
        "* The amount of clips, can be configured by changing top_n paramater in the config file\n",
        "* The current threshold, is configured as to take the top 1 percentile sliding window, it is recommended to run the code till visualization, which lists the probablity vs time distribution, and configuring the threshold there only, then running the remaining steps.\n",
        "* The sliding window is set to 30 seconds, which will produce clips of that length only, to change, you can modify window_sec paramater, but MAKE SURE it is below 60 seconds\n",
        "* The checkpoint directory, contained within the base path- contains the necesary intermediate files, including the raw audio, seperated audio in \"chunks\", log mel spectrogram, and raw predictions for all the sliding windows as well as the metadata for the final clips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpXdcXZGorS"
      },
      "source": [
        "**1. Installing additional dependencies, other than what colab already has**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PPncxLzGorT",
        "outputId": "35118ce2-d14d-4e0a-b2ce-e442e213de7c"
      },
      "outputs": [],
      "source": [
        "!pip install -q yt-dlp demucs librosa tqdm torchcodec\n",
        "\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sATa2hWLGorT"
      },
      "source": [
        "**2. Setting up Base path, currently it is set at the local storage of colab runtime, you can change accordingly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJmdLoIEGorT"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "BASE_PATH = '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmt8OEb_E2e1"
      },
      "source": [
        "**3. Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNZxtnqKGorU",
        "outputId": "2193b79a-5a42-485e-d7ac-5ce11d4e2c10"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # main inputs\n",
        "    twitch_url: str = \"https://www.twitch.tv/videos/2657723379\"\n",
        "    model_path: str = \"/content/best_model.pt\"\n",
        "\n",
        "    # output folders\n",
        "    output_dir: str = os.path.join(BASE_PATH, \"HighlightOutput_inference\")\n",
        "    checkpoint_dir: str = os.path.join(BASE_PATH, \"HighlightOutput_inference\", \"checkpoints\")\n",
        "\n",
        "    # audio settings\n",
        "    sample_rate: int = 22050\n",
        "    n_mels: int = 80\n",
        "    hop_length: int = 512\n",
        "\n",
        "    # inference settings\n",
        "    window_sec: float = 30.0\n",
        "    stride_sec: float = 5.0\n",
        "    threshold_percentile: float = 99 # in percentage\n",
        "    nms_window: float = 10.0\n",
        "    top_n: int = 20\n",
        "\n",
        "    # model settings\n",
        "    cnn_channels = [32, 64, 128, 256]\n",
        "    rnn_hidden: int = 256\n",
        "    rnn_layers: int = 2\n",
        "    rnn_dropout: float = 0.3\n",
        "    attention_dim: int = 128\n",
        "    dropout: float = 0.5\n",
        "\n",
        "    def __post_init__(self):\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    @property\n",
        "    def window_frames(self):\n",
        "        return int((self.window_sec * self.sample_rate) / self.hop_length) + 1\n",
        "\n",
        "    @property\n",
        "    def stride_frames(self):\n",
        "        return int((self.stride_sec * self.sample_rate) / self.hop_length)\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "print(\"Config Loaded\")\n",
        "print(\"VOD URL:\", cfg.twitch_url)\n",
        "print(\"Model Path:\", cfg.model_path)\n",
        "print(\"Output Folder:\", cfg.output_dir)\n",
        "print(\"Checkpoints Folder:\", cfg.checkpoint_dir)\n",
        "print(\"Window Seconds:\", cfg.window_sec, \"Frames:\", cfg.window_frames)\n",
        "print(\"Stride Seconds:\", cfg.stride_sec, \"Frames:\", cfg.stride_frames)\n",
        "print(\"Threshold:\", cfg.threshold_percentile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranOnxiQGorV"
      },
      "source": [
        "**4. Downloading audio file** using ffmpeg to download the audio and saving it in the checkpoint directory, as vod_audio.m4a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIqcYn24GorV",
        "outputId": "57ad6249-2e0a-4264-8297-4ebf4a7c02fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Step 3: Downloading Audio\")\n",
        "\n",
        "audio_path = os.path.join(cfg.checkpoint_dir, \"vod_audio.m4a\")\n",
        "\n",
        "# Check if already downloaded\n",
        "if os.path.exists(audio_path):\n",
        "    print(\"Audio already exists:\", audio_path)\n",
        "else:\n",
        "    print(\"Downloading audio...\")\n",
        "    !yt-dlp -f \"Audio_Only\" -o \"{audio_path}\" \"{cfg.twitch_url}\"\n",
        "print(\"Step 3 done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqvUtCEBGorW"
      },
      "source": [
        "**5. Running seperation through demucs**- this part divides the long VOD to minute long chunks for processing by demucs, after which they are patched together and saved in the checkpoint directory folder as vocals.wav and game.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIUD_NclGorW",
        "outputId": "86d0958b-8966-4bb9-a9d1-6152c07bd139"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "print(\"Chunked Audio Separation\")\n",
        "\n",
        "audio_path = os.path.join(cfg.checkpoint_dir, \"vod_audio.m4a\")\n",
        "vocals_path = os.path.join(cfg.checkpoint_dir, \"vocals.wav\")\n",
        "game_path = os.path.join(cfg.checkpoint_dir, \"game.wav\")\n",
        "\n",
        "# skip if already processed\n",
        "if os.path.exists(vocals_path) and os.path.exists(game_path):\n",
        "    print(\"Already separated. Skipping.\")\n",
        "    print(\"Vocals:\", vocals_path)\n",
        "    print(\"Game:\", game_path)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(audio_path):\n",
        "        raise RuntimeError(\"Audio file not found.\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    chunks_dir = os.path.join(cfg.checkpoint_dir, \"chunks_temp\")\n",
        "    out_dir = os.path.join(cfg.checkpoint_dir, \"demucs_temp\")\n",
        "\n",
        "    os.makedirs(chunks_dir, exist_ok=True)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # split audio into 60s chunks\n",
        "    print(\"Splitting audio into chunks...\")\n",
        "    chunk_pattern = os.path.join(chunks_dir, \"chunk_%03d.wav\")\n",
        "\n",
        "    subprocess.run([\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\", audio_path,\n",
        "    \"-f\", \"segment\",\n",
        "    \"-segment_time\", \"60\",\n",
        "    \"-acodec\", \"pcm_s16le\",\n",
        "    \"-ar\", \"44100\",\n",
        "    \"-ac\", \"2\",\n",
        "    chunk_pattern\n",
        "    ], check=True)\n",
        "\n",
        "    chunk_files = sorted([os.path.join(chunks_dir, f) for f in os.listdir(chunks_dir)])\n",
        "\n",
        "    if len(chunk_files) == 0:\n",
        "        raise RuntimeError(\"No chunks created.\")\n",
        "\n",
        "    print(\"Running Demucs on chunks...\")\n",
        "    for i,chunk in enumerate(chunk_files):\n",
        "      print(f\"running demucs on {i}th chunk\")\n",
        "      subprocess.run([\n",
        "            \"python\", \"-m\", \"demucs\",\n",
        "            \"--two-stems\", \"vocals\",\n",
        "            \"-n\", \"htdemucs\",\n",
        "            \"-d\", device,\n",
        "            \"--jobs\", \"1\",\n",
        "            \"-o\", out_dir,\n",
        "            chunk\n",
        "        ])\n",
        "\n",
        "    # collect separated files\n",
        "    vocals_list = []\n",
        "    game_list = []\n",
        "\n",
        "    for chunk in chunk_files:\n",
        "        name = os.path.splitext(os.path.basename(chunk))[0]\n",
        "        folder = os.path.join(out_dir, \"htdemucs\", name)\n",
        "\n",
        "        v = os.path.join(folder, \"vocals.wav\")\n",
        "        g = os.path.join(folder, \"no_vocals.wav\")\n",
        "\n",
        "        if os.path.exists(v) and os.path.exists(g):\n",
        "            vocals_list.append(v)\n",
        "            game_list.append(g)\n",
        "\n",
        "    if len(vocals_list) == 0 or len(game_list) == 0:\n",
        "        raise RuntimeError(\"Demucs outputs missing.\")\n",
        "\n",
        "    # helper to merge\n",
        "    def merge(files, out_file):\n",
        "        txt = os.path.join(cfg.checkpoint_dir, \"list.txt\")\n",
        "        with open(txt, \"w\") as f:\n",
        "            for x in files:\n",
        "                f.write(f\"file '{x}'\\n\")\n",
        "\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-y\",\n",
        "            \"-f\", \"concat\",\n",
        "            \"-safe\", \"0\",\n",
        "            \"-i\", txt,\n",
        "            \"-c\", \"copy\",\n",
        "            out_file\n",
        "        ])\n",
        "\n",
        "    print(\"Merging vocals...\")\n",
        "    merge(vocals_list, vocals_path)\n",
        "\n",
        "    print(\"Merging game audio..\")\n",
        "    merge(game_list, game_path)\n",
        "\n",
        "    print(\"Vocals:\", vocals_path)\n",
        "    print(\"Game:\", game_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fkuMyqeGorX"
      },
      "source": [
        "**6. Converting to log mel spectrograms and stacking**  This converts the raw vocals and game wav files to stacked pytorch tensors, saved as spectrogram.pt in the checkpoint directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrUDgY6GorX",
        "outputId": "e7d88a3e-5f43-4d95-ad5f-f52c97e2b74a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "\n",
        "SPEC_CHECKPOINT = os.path.join(cfg.checkpoint_dir, \"spectrogram.pt\")\n",
        "vocals_path = os.path.join(cfg.checkpoint_dir, \"vocals.wav\")\n",
        "game_path = os.path.join(cfg.checkpoint_dir, \"game.wav\")\n",
        "\n",
        "\n",
        "# If already created, load it\n",
        "if os.path.exists(SPEC_CHECKPOINT):\n",
        "    print(\"Found saved spectrogram.\")\n",
        "    spectrogram = torch.load(SPEC_CHECKPOINT)\n",
        "\n",
        "    frames = spectrogram.shape[2]\n",
        "    duration_sec = frames * cfg.hop_length / cfg.sample_rate\n",
        "\n",
        "    print(\"Loaded successfully\")\n",
        "    print(\"Duration (minutes):\", duration_sec / 60)\n",
        "\n",
        "else:\n",
        "  # function to convert to log mel spectrograms\n",
        "    def audio_to_melspec(path, sr, n_mels, hop_length, label):\n",
        "        print(f\"Processing {label}\")\n",
        "        y, _ = librosa.load(path, sr=sr, mono=True)\n",
        "\n",
        "        print(\"Audio length (seconds):\", len(y) / sr)\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            n_mels=n_mels,\n",
        "            hop_length=hop_length\n",
        "        )\n",
        "\n",
        "        mel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        print(\"Mel shape:\", mel.shape)\n",
        "        return mel\n",
        "\n",
        "    # Create mel specs\n",
        "    vocals_mel = audio_to_melspec(\n",
        "        vocals_path,\n",
        "        cfg.sample_rate,\n",
        "        cfg.n_mels,\n",
        "        cfg.hop_length,\n",
        "        \"vocals\"\n",
        "    )\n",
        "\n",
        "    game_mel = audio_to_melspec(\n",
        "        game_path,\n",
        "        cfg.sample_rate,\n",
        "        cfg.n_mels,\n",
        "        cfg.hop_length,\n",
        "        \"game audio\"\n",
        "    )\n",
        "\n",
        "    # Match lengths\n",
        "    length = min(vocals_mel.shape[1], game_mel.shape[1])\n",
        "    vocals_mel = vocals_mel[:, :length]\n",
        "    game_mel = game_mel[:, :length]\n",
        "\n",
        "    # Stack- final shape (2, n_mels, frames)\n",
        "    spectrogram = np.stack([vocals_mel, game_mel], axis=0)\n",
        "    spectrogram = torch.from_numpy(spectrogram).float()\n",
        "\n",
        "    # Save\n",
        "    torch.save(spectrogram, SPEC_CHECKPOINT)\n",
        "\n",
        "    duration_sec = length * cfg.hop_length / cfg.sample_rate\n",
        "\n",
        "    print(\"Saved spectrogram\")\n",
        "    print(\"Path:\", SPEC_CHECKPOINT)\n",
        "    print(\"Shape:\", spectrogram.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYrFz2XrGorY"
      },
      "source": [
        "**7. Loading the model** uses the trained HighlightSense Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBqOjOH1GorY",
        "outputId": "73ca71ad-2a93-4006-d2d5-52f5f4f86c96"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#conv block\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, inc, outc, pool=(2,2), first_layer=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(inc, outc, 3, padding=1)\n",
        "\n",
        "        if first_layer:\n",
        "            self.bn = nn.InstanceNorm2d(outc, affine=True)\n",
        "        else:\n",
        "            self.bn = nn.BatchNorm2d(outc)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(pool)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#attn layer define\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, att_dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dim, att_dim)\n",
        "        self.V = nn.Linear(att_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        e = torch.tanh(self.W(x))\n",
        "        s = self.V(e).squeeze(-1)\n",
        "\n",
        "        if mask is not None:\n",
        "            s = s.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        w = F.softmax(s, dim=1)\n",
        "        ctx = torch.bmm(w.unsqueeze(1), x).squeeze(1)\n",
        "        return ctx, w\n",
        "\n",
        "\n",
        "\n",
        "# Main Model\n",
        "class HighlightSenseModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_mels=80,\n",
        "        cnn_channels=[32,64,128,256],\n",
        "        rnn_hidden=256,\n",
        "        rnn_layers=2,\n",
        "        rnn_dropout=0.3,\n",
        "        att_dim=128,\n",
        "        dropout=0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_ch = 2\n",
        "\n",
        "        # CNN\n",
        "        for i, out_ch in enumerate(cnn_channels):\n",
        "            pool = (2,2) if i < 3 else (1,2)\n",
        "            first = (i == 0)\n",
        "            layers.append(ConvBlock(in_ch, out_ch, pool, first_layer=first))\n",
        "            in_ch = out_ch\n",
        "\n",
        "        self.cnn = nn.Sequential(*layers)\n",
        "\n",
        "        freq_out = n_mels // 8\n",
        "        rnn_input = cnn_channels[-1] * freq_out\n",
        "\n",
        "        # RNN\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=rnn_input,\n",
        "            hidden_size=rnn_hidden,\n",
        "            num_layers=rnn_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=rnn_dropout if rnn_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "     #attention+classifier\n",
        "        self.attention = Attention(rnn_hidden * 2, att_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_hidden * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout / 2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B = x.size(0)\n",
        "\n",
        "        c = self.cnn(x)\n",
        "        c = c.permute(0, 3, 1, 2)\n",
        "        c = c.reshape(B, c.size(1), -1)\n",
        "\n",
        "        if mask is not None:\n",
        "            m = mask[:, ::16]\n",
        "            m = m[:, :c.size(1)]\n",
        "        else:\n",
        "            m = None\n",
        "\n",
        "        r, _ = self.rnn(c)\n",
        "        ctx, att = self.attention(r, m)\n",
        "        out = self.classifier(ctx)\n",
        "\n",
        "        return out, att\n",
        "\n",
        "\n",
        "# Create Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = HighlightSenseModel(\n",
        "    n_mels=cfg.n_mels,\n",
        "    cnn_channels=cfg.cnn_channels,\n",
        "    rnn_hidden=cfg.rnn_hidden,\n",
        "    rnn_layers=cfg.rnn_layers,\n",
        "    rnn_dropout=cfg.rnn_dropout,\n",
        "    att_dim=cfg.attention_dim,\n",
        "    dropout=cfg.dropout\n",
        ").to(device)\n",
        "#loading weights\n",
        "print(f\"Loading weights from: {cfg.model_path}\")\n",
        "\n",
        "checkpoint = torch.load(cfg.model_path, map_location=device,weights_only=False)\n",
        "state_dict = checkpoint['model']\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "print(\"Model weights loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzp2VVqmGorZ"
      },
      "source": [
        "**8. Sliding window** This runs the sliding window, through the vod and then pads the clips to 60 seconds, using pad_value as the lowest energy time step derived from the raw clip, as to match the training setup. raw_window_scores is the csv file containing the index, starting time step, ending time step and the model confidence out of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "8JGLg2NZGorZ",
        "outputId": "7239a266-ba9d-47f0-aeaa-fca0b5e25100"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import os\n",
        "\n",
        "output_csv = os.path.join(cfg.checkpoint_dir, \"raw_window_scores.csv\")\n",
        "results=[]\n",
        "raw_results=[]\n",
        "\n",
        "print(\"Window seconds:\", cfg.window_sec)\n",
        "print(\"Stride seconds:\", cfg.stride_sec)\n",
        "print(\"Threshold percentile:\", cfg.threshold_percentile)\n",
        "\n",
        "total_frames = spectrogram.shape[2]\n",
        "window_frames = cfg.window_frames\n",
        "stride_frames = cfg.stride_frames\n",
        "# all clips padded to target frames that is 60 seconds\n",
        "target_frames = int((60 * cfg.sample_rate) / cfg.hop_length)\n",
        "num_windows = max(1, (total_frames - window_frames) // stride_frames + 1)\n",
        "print(\"Total windows:\", num_windows)\n",
        "\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(num_windows):\n",
        "        start_frame = i * stride_frames\n",
        "        end_frame = start_frame + window_frames\n",
        "\n",
        "        if end_frame > total_frames:\n",
        "            end_frame = total_frames\n",
        "            start_frame = max(0, end_frame - window_frames)\n",
        "#padding to 60 second window size 30 to match training input\n",
        "        window = spectrogram[:, :, start_frame:end_frame]\n",
        "        actual_frames = window.shape[2]\n",
        "        pad_needed = target_frames - actual_frames\n",
        "        if pad_needed > 0:\n",
        "          pad_value = window.min().item()\n",
        "          window = F.pad(window, (0, pad_needed), value=pad_value)\n",
        "\n",
        "# Mask\n",
        "        mask = torch.zeros(target_frames)\n",
        "        mask[:actual_frames] = 1\n",
        "        window = window.unsqueeze(0).to(device)\n",
        "        mask = mask.unsqueeze(0).to(device)\n",
        "\n",
        "        logits, _ = model(window, mask)\n",
        "        prob = torch.sigmoid(logits).item()\n",
        "        start_sec = start_frame * cfg.hop_length / cfg.sample_rate\n",
        "        end_sec = end_frame * cfg.hop_length / cfg.sample_rate\n",
        "        raw_results.append([i,start_sec,end_sec,prob])\n",
        "\n",
        "print(\"Writing CSV:\", output_csv)\n",
        "with open(output_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"window_index\", \"start_sec\", \"end_sec\", \"confidence\"])\n",
        "    writer.writerows(raw_results)\n",
        "\n",
        "print(\"Total windows scored:\", len(raw_results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXPHCQ74FjJ"
      },
      "source": [
        "**9. Visualisation and modification** the first part in this section, first run the 1st part and 2nd part, check the graph plotted, the dotted red line represents the threshold probablity, any sliding window having probablity below that percentile wont be considered, lower the the percentile to have higher amount of clips, but having lower confidence score, and higher to filter out only the top clips, to change the percentile from here only, uncomment the threshold_percentile, and change it to your preference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX-y2E1DiS1z",
        "outputId": "e7de81e7-b3b9-4cb2-dcd1-715b82b6fbc7"
      },
      "outputs": [],
      "source": [
        "threshold_percentile=cfg.threshold_percentile\n",
        "#---------------------------------------------------------------------------------------------------------------\n",
        "# CURRENTLY THRESHOLD PERCENTILE IS SET AT 99, uncomment below to change it according to the visualisation below\n",
        "# threshold_percentile=99.9\n",
        "#---------------------------------------------------------------------------------------------------------------\n",
        "probablity_calc = [row[3] for row in raw_results]\n",
        "new_threshold=np.percentile(probablity_calc,threshold_percentile)\n",
        "for row in raw_results:\n",
        "  r_idx = row[0]\n",
        "  r_start = row[1]\n",
        "  r_end = row[2]\n",
        "  r_prob = row[3]\n",
        "  if r_prob >= new_threshold:\n",
        "   results.append([r_idx,r_start,r_end,r_prob])\n",
        "print(\"Windows above threshold:\", len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "smrV4vgFNFKx",
        "outputId": "40bbbbd3-70ed-4d93-e73f-679dcb8d9ed5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# reading raw csv\n",
        "csv_path = os.path.join(cfg.checkpoint_dir, \"raw_window_scores.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# plot time (x) vs confidence (y)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df['start_sec'] / 60, df['confidence'], label='Prob')\n",
        "plt.axhline(y=new_threshold, color='red', linestyle='--', label='Threshold')\n",
        "\n",
        "plt.xlabel('Time (min)')\n",
        "plt.ylabel('Confidence')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8lPPeqbGorZ"
      },
      "source": [
        "**10. Running NMS Supression** running the NMS script on the clips above the threshold, and discarding similar clips, which have a high intersection, keeping only 1 copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Zr15kLGorZ",
        "outputId": "798512e6-bff5-421b-c56f-0dfd0df17170"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def apply_nms(detections, nms_window):\n",
        "    if not detections:\n",
        "        return []\n",
        "\n",
        "    # sort by confidence (highest first)\n",
        "    detections = sorted(detections, key=lambda x: x[3], reverse=True)\n",
        "    kept = []\n",
        "    removed = set()\n",
        "\n",
        "    for i, det in enumerate(detections):\n",
        "        if i in removed:\n",
        "            continue\n",
        "\n",
        "        kept.append(det)\n",
        "\n",
        "        for j, other in enumerate(detections[i+1:], start=i+1):\n",
        "            if j in removed:\n",
        "                continue\n",
        "\n",
        "            # time values\n",
        "            det_start = det[1]\n",
        "            det_end = det[2]\n",
        "\n",
        "            other_start = other[1]\n",
        "            other_end = other[2]\n",
        "\n",
        "            # overlap check\n",
        "            overlap_start = max(det_start, other_start)\n",
        "            overlap_end = min(det_end, other_end)\n",
        "            overlaps = overlap_end > overlap_start\n",
        "\n",
        "            # also treat \"very close\" clips as duplicates\n",
        "            close_start = abs(det_start - other_start) < nms_window\n",
        "\n",
        "            if overlaps or close_start:\n",
        "                removed.add(j)\n",
        "\n",
        "    # sort by start time for clean output\n",
        "    return sorted(kept, key=lambda x: x[1])\n",
        "\n",
        "\n",
        "def to_timestamp(sec):\n",
        "    h = int(sec // 3600)\n",
        "    m = int((sec % 3600) // 60)\n",
        "    s = int(sec % 60)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "\n",
        "print(\"NMS window:\", cfg.nms_window)\n",
        "final_detections = apply_nms(results, cfg.nms_window)\n",
        "\n",
        "print(\"After NMS:\", len(results), \"to\", len(final_detections))\n",
        "\n",
        "if not final_detections:\n",
        "    print(\"No highlights found\")\n",
        "\n",
        "else:\n",
        "    # sort again by confidence and take top N\n",
        "    top = sorted(final_detections, key=lambda x: x[3], reverse=True)[:cfg.top_n]\n",
        "\n",
        "    print(\"\\nTop detections:\")\n",
        "    print(\"Rank | Start | End | Duration | Confidence\")\n",
        "\n",
        "    for i, det in enumerate(top):\n",
        "        start = to_timestamp(det[1])\n",
        "        end = to_timestamp(det[2])\n",
        "        duration = det[2] - det[1]\n",
        "        print(f\"{i+1} {start} {end} {duration:.1f}s {det[3]:.3f}\")\n",
        "\n",
        "    # Fix: Open file here and write loop separately\n",
        "    with open('highlight_results.csv', 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        \n",
        "        for i, det in enumerate(top):\n",
        "            writer.writerow([\n",
        "                i+1,\n",
        "                det[1],\n",
        "                det[2],\n",
        "                to_timestamp(det[1]),\n",
        "                to_timestamp(det[2]),\n",
        "                det[2] - det[1],\n",
        "                det[3]\n",
        "            ])\n",
        "print(\"Step 8 complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBqwGBg0Gora"
      },
      "source": [
        "**11. Extracting video clips** extracting the clips, directly from the source using FFMPEG, the final clip metadata is saved as meta_csv in the checkpoint folder, containing metadata about the clip path, duration, confidence score, and time stamps. The clips are stored as highlight_0i.mp4 in the clips directory in base folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tywfa4fAGora",
        "outputId": "33f4dda6-422e-4f14-fab8-d4bac2258cbd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import subprocess\n",
        "\n",
        "print(\"Step 9: Extracting Video Clips\")\n",
        "\n",
        "CLIPS_DIR = os.path.join(cfg.output_dir, \"clips\")\n",
        "os.makedirs(CLIPS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Getting Twitch stream URL...\")\n",
        "stream_url = subprocess.check_output([\n",
        "    \"yt-dlp\", \"-g\", \"-f\", \"best\", cfg.twitch_url\n",
        "]).decode().strip()\n",
        "\n",
        "def to_timestamp(sec):\n",
        "    h = int(sec // 3600)\n",
        "    m = int((sec % 3600) // 60)\n",
        "    s = int(sec % 60)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "TOP_N = min(cfg.top_n, len(final_detections))\n",
        "print(\"Extracting top\", TOP_N, \"clips\")\n",
        "print(\"Saving clips to:\", CLIPS_DIR)\n",
        "\n",
        "top = sorted(final_detections, key=lambda x: x[3], reverse=True)[:TOP_N]\n",
        "\n",
        "clip_info = []\n",
        "\n",
        "for i, det in enumerate(top):\n",
        "    start_sec = max(0, det[1])\n",
        "    end_sec = det[2]\n",
        "    duration = end_sec - det[1]\n",
        "\n",
        "    clip_path = os.path.join(CLIPS_DIR, f\"highlight_{i+1:02d}.mp4\")\n",
        "\n",
        "    print(\"Clip\", i+1, \"start:\", to_timestamp(start_sec), \"duration:\", round(duration))\n",
        "\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "        \"-ss\", str(start_sec),\n",
        "        \"-i\", stream_url,\n",
        "        \"-t\", str(duration),\n",
        "        \"-c\", \"copy\",\n",
        "        clip_path\n",
        "    ])\n",
        "\n",
        "    clip_info.append([\n",
        "        i+1,\n",
        "        det[1],\n",
        "        det[2],\n",
        "        to_timestamp(det[1]),\n",
        "        to_timestamp(det[2]),\n",
        "        det[2] - det[1],\n",
        "        det[3],\n",
        "        clip_path\n",
        "    ])\n",
        "\n",
        "meta_csv = os.path.join(cfg.output_dir, \"final_clips_metadata.csv\")\n",
        "with open(meta_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\n",
        "        \"rank\", \"start_sec\", \"end_sec\",\n",
        "        \"start_ts\", \"end_ts\",\n",
        "        \"duration_sec\", \"confidence\", \"clip_path\"\n",
        "    ])\n",
        "    writer.writerows(clip_info)\n",
        "\n",
        "print(\"Step 9 completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlV1i8jJVxDO"
      },
      "source": [
        "**12. Cleanup** to run another vod, first uncomment out the below code, which cleans the output directory, and before running script from the start comment it again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1QOMWLPGorb"
      },
      "outputs": [],
      "source": [
        "#import shutil; shutil.rmtree(cfg.output_dir, ignore_errors=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
